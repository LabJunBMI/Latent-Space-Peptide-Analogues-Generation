{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import EsmModel, EsmTokenizer\n",
    "from transformers.models.esm.modeling_esm import EsmEncoder, EsmConfig\n",
    "\n",
    "gc.enable()\n",
    "# ID of GPU to use\n",
    "GPU_ID = 0\n",
    "device = torch.device(f'cuda:{GPU_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "# The number of sequences to generate\n",
    "GEN_NUM = 3\n",
    "# ESM encoder model name\n",
    "ESM_MODE = \"facebook/esm2_t30_150M_UR50D\"\n",
    "# Decoder model path\n",
    "DECODER_MODEL = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with encoder and decoder parts\n",
    "class EsmEncoderDecoderModel(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, encoder:EsmModel, learning_rate=5e-5):\n",
    "        super(EsmEncoderDecoderModel, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Create decoder using the provided configuration\n",
    "        decoder_config = encoder.config.to_dict()\n",
    "        decoder_config[\"num_hidden_layers\"] = 2\n",
    "        # decoder_config[\"intermediate_size\"] = 640\n",
    "        self.decoder = EsmEncoder(EsmConfig(**decoder_config))\n",
    "        # Add a linear layer to match the decoder output shape to the encoder input shape\n",
    "        self.output_to_vocab = nn.Linear(decoder_config[\"hidden_size\"], decoder_config[\"vocab_size\"])\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Pass the input through the encoder\n",
    "        encoder_outputs = self.encoder(input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        # Pass the encoder outputs through the decoder\n",
    "        extended_attention_mask = self.encoder.get_extended_attention_mask(attention_mask, input_ids.size())\n",
    "        decoder_outputs = self.decoder(encoder_outputs, attention_mask=extended_attention_mask).last_hidden_state\n",
    "        # Transform the decoder outputs to match the input shape of the tokenizer\n",
    "        transformed_outputs = self.output_to_vocab(decoder_outputs)\n",
    "        return transformed_outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask = batch\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask)\n",
    "        # Calculate loss (example using CrossEntropyLoss)\n",
    "        print(outputs.view(-1, outputs.size(-1)), outputs.view(-1, outputs.size(-1)).shape)\n",
    "        print(input_ids.view(-1), input_ids.view(-1).shape)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), input_ids.view(-1), ignore_index=self.tokenizer.pad_token_id)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask = batch\n",
    "        # Forward pass\n",
    "        outputs = self(input_ids, attention_mask=attention_mask)\n",
    "        # Calculate loss (example using CrossEntropyLoss)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), input_ids.view(-1), ignore_index=self.tokenizer.pad_token_id)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_repeated_AA(s: str, threshold: float=0.5) -> bool:\n",
    "    # Calculate the threshold count\n",
    "    threshold_count = len(s) * threshold\n",
    "    # Create a dictionary to count occurrences of each character\n",
    "    char_count = {}\n",
    "    # Count occurrences of each character\n",
    "    for char in s:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    # Check if any character exceeds the threshold count\n",
    "    for count in char_count.values():\n",
    "        if count > threshold_count:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_consecutive_AA(s: str, threshold: float=0.3) -> bool:\n",
    "    # Calculate the threshold count\n",
    "    threshold_count = len(s) * threshold\n",
    "    # Initialize variables to track the current character and its consecutive count\n",
    "    max_consecutive_count = 0\n",
    "    current_char = ''\n",
    "    current_consecutive_count = 0\n",
    "    # Iterate through the string to count consecutive characters\n",
    "    for char in s:\n",
    "        if char == current_char:\n",
    "            current_consecutive_count += 1\n",
    "        else:\n",
    "            current_char = char\n",
    "            current_consecutive_count = 1\n",
    "        # Update the max consecutive count\n",
    "        if current_consecutive_count > max_consecutive_count:\n",
    "            max_consecutive_count = current_consecutive_count\n",
    "    # Check if the max consecutive count exceeds the threshold count\n",
    "    return max_consecutive_count > threshold_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_tokenizer(encoder_model_name):\n",
    "    # Load the pre-trained ESM model\n",
    "    encoder_model = EsmModel.from_pretrained(encoder_model_name)\n",
    "    # Load the tokenizer\n",
    "    tokenizer = EsmTokenizer.from_pretrained(encoder_model_name)\n",
    "    model = EsmEncoderDecoderModel(tokenizer, encoder_model)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liangpu/.conda/envs/torch2.3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t30_150M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = get_model_tokenizer(ESM_MODE)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(DECODER_MODEL))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158664954"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(seq):\n",
    "    token = tokenizer(\n",
    "        seq, return_tensors=\"pt\", \n",
    "        max_length=256, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "    extended_attention_mask = model.encoder.get_extended_attention_mask(\n",
    "        token[\"attention_mask\"].to(device), \n",
    "        token[\"input_ids\"].size()\n",
    "    )\n",
    "    embedding = model.encoder(\n",
    "        token[\"input_ids\"].to(device), \n",
    "        attention_mask=token[\"attention_mask\"].to(device),\n",
    "    ).last_hidden_state \n",
    "    return embedding, extended_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 640])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = \"CNCKRFPQCPLNFLC\"\n",
    "embedding, mask = get_embedding(input_seq)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(m:torch.Tensor, noise_scale:float):\n",
    "    # Add noise to the hidden states\n",
    "    noise = torch.randn_like(m)\n",
    "    noise = noise - noise.min()  # Shift noise to be non-negative\n",
    "    noise = noise / noise.max()  # Normalize noise to [0, 1]\n",
    "    noise = noise * 2 - 1  # Shift noise to [-1, 1]\n",
    "    noise += torch.rand(1).item()*2-1 # Add a random shift to the noise\n",
    "    noise = noise * noise_scale  # Adjust the scale of noise as needed\n",
    "    noised_embedding = m + noise\n",
    "    noised_embedding = noised_embedding.to(m.device)\n",
    "    return noised_embedding\n",
    "\n",
    "def generate_seq(\n",
    "    input_seq, embedding, extended_attention_mask, gen_num=3,\n",
    "    noise_start=0.5, noise_step=0.1, noise_timestep=2000, time_limit=10000\n",
    "):\n",
    "    total_start = time()\n",
    "    total_step=0\n",
    "    # Load the model and tokenizer\n",
    "    noise_add = 0\n",
    "    i=0\n",
    "    res=[]\n",
    "    step=0\n",
    "    \n",
    "    while i<gen_num:\n",
    "        # Add noise to the hidden states\n",
    "        noised_embedding = add_noise(\n",
    "            embedding, (noise_start+(noise_add*noise_step)))\n",
    "        # Prepare the encoder outputs with the noised hidden states\n",
    "        decoder_outputs = model.decoder(\n",
    "            noised_embedding, \n",
    "            attention_mask=extended_attention_mask\n",
    "        ).last_hidden_state\n",
    "        # transform the hidden state to vocab\n",
    "        transformed_outputs = model.output_to_vocab(decoder_outputs)\n",
    "        pred_ids = torch.functional.F.softmax(transformed_outputs, 2)[0].argmax(axis=1)\n",
    "        output_seq = \"\".join([tokenizer.all_tokens[i] for i in pred_ids][1:len(input_seq)+1])\n",
    "        is_input_weird = has_consecutive_AA(input_seq) or (has_repeated_AA(input_seq))\n",
    "        is_weird = has_consecutive_AA(output_seq) or (has_repeated_AA(output_seq))\n",
    "        is_weird = is_weird and (not is_input_weird)# if input is weird, the resuld could be weird\n",
    "        if (output_seq != input_seq) and (output_seq not in res) and (not is_weird):\n",
    "            print(\"Noised and Reconstructed Output:\", output_seq)\n",
    "            res.append(output_seq)\n",
    "            i+=1\n",
    "            step=0\n",
    "        if(step>noise_timestep):\n",
    "            noise_add+=1\n",
    "            step = 0\n",
    "            print(f\"Increasing Noise to: {noise_start+(noise_add*noise_step)}\")\n",
    "        if(step>time_limit):\n",
    "            print(\"Reach time limit\")\n",
    "            break\n",
    "        step+=1\n",
    "        total_step+=1\n",
    "        del noised_embedding\n",
    "    print(\"Original Input:\", input_seq)\n",
    "    print(\"Time cost:\", time()-total_start)\n",
    "    print(\"Total step:\", total_step)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = \"CNCKRFPQCPLNFLC\"\n",
    "embedding, mask = get_embedding(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increasing Noise to: 0.6\n",
      "Increasing Noise to: 0.7\n",
      "Increasing Noise to: 0.8\n",
      "Noised and Reconstructed Output: CNCTRFPQCPLNFLC\n",
      "Noised and Reconstructed Output: CNCKRFPQCPLNFLS\n",
      "Noised and Reconstructed Output: CNCKRFPQCPLNFLR\n",
      "Original Input: CNCKRFPQCPLNFLC\n",
      "Time cost: 35.50789141654968\n",
      "Total step: 6249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CNCTRFPQCPLNFLC', 'CNCKRFPQCPLNFLS', 'CNCKRFPQCPLNFLR']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_seq(input_seq, embedding, mask, gen_num=GEN_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
