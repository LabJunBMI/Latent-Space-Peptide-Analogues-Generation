{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import re\n",
    "gc.enable()\n",
    "# ID of GPU to use\n",
    "GPU_ID = 0\n",
    "device = torch.device(f'cuda:{GPU_ID}' if torch.cuda.is_available() else 'cpu')\n",
    "# The number of sequences to generate\n",
    "GEN_NUM = 3\n",
    "# Model to use\n",
    "MODEL_PATH = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2818830336"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_repeated_AA(s: str, threshold: float=0.5) -> bool:\n",
    "    # Calculate the threshold count\n",
    "    threshold_count = len(s) * threshold\n",
    "    # Create a dictionary to count occurrences of each character\n",
    "    char_count = {}\n",
    "    # Count occurrences of each character\n",
    "    for char in s:\n",
    "        if char in char_count:\n",
    "            char_count[char] += 1\n",
    "        else:\n",
    "            char_count[char] = 1\n",
    "    # Check if any character exceeds the threshold count\n",
    "    for count in char_count.values():\n",
    "        if count > threshold_count:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_consecutive_AA(s: str, threshold: float=0.3) -> bool:\n",
    "    # Calculate the threshold count\n",
    "    threshold_count = len(s) * threshold\n",
    "    # Initialize variables to track the current character and its consecutive count\n",
    "    max_consecutive_count = 0\n",
    "    current_char = ''\n",
    "    current_consecutive_count = 0\n",
    "    # Iterate through the string to count consecutive characters\n",
    "    for char in s:\n",
    "        if char == current_char:\n",
    "            current_consecutive_count += 1\n",
    "        else:\n",
    "            current_char = char\n",
    "            current_consecutive_count = 1\n",
    "        # Update the max consecutive count\n",
    "        if current_consecutive_count > max_consecutive_count:\n",
    "            max_consecutive_count = current_consecutive_count\n",
    "    # Check if the max consecutive count exceeds the threshold count\n",
    "    return max_consecutive_count > threshold_count\n",
    "\n",
    "def get_embedding(seq:str):\n",
    "    #\"C N C K R F P Q C P L N F L C\"\n",
    "    # Define your input\n",
    "    sequences_Example = [\" \".join(seq)]\n",
    "    sequences_Example = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequences_Example]\n",
    "    input_seq = sequences_Example[0]\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(input_seq, add_special_tokens=True, padding=True, return_tensors=\"pt\")\n",
    "    tokens = tokens.to(device)\n",
    "\n",
    "    # Pass the input through the encoder\n",
    "    encoder_outputs = model.encoder(\n",
    "        input_ids=tokens.input_ids,\n",
    "        attention_mask=tokens.attention_mask\n",
    "    )\n",
    "    # Extract the hidden states\n",
    "    encoder_hidden_states = encoder_outputs.last_hidden_state\n",
    "    del tokens, encoder_outputs\n",
    "    return encoder_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding(\"CNCKRFPQCPLNFLC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(m:torch.Tensor, noise_scale:float, fix_pos:list=None):\n",
    "    # Add noise to the hidden states\n",
    "    noise = torch.randn_like(m)\n",
    "    noise = noise - noise.min()  # Shift noise to be non-negative\n",
    "    noise = noise / noise.max()  # Normalize noise to [0, 1]\n",
    "    noise = noise * 2 - 1  # Shift noise to [-1, 1]\n",
    "    noise += torch.rand(1).item()*2-1 # Add a random shift to the noise\n",
    "    noise = noise * noise_scale  # Adjust the scale of noise as needed\n",
    "    # Create mask for positions to add noise\n",
    "    if(fix_pos):\n",
    "        seq_length = m.shape[1]\n",
    "        mask = torch.ones(seq_length, dtype=torch.float, device=device)\n",
    "        mask[fix_pos] = 0.0\n",
    "        noise *= mask.view(1,-1,1)\n",
    "    noised_embedding = m + noise\n",
    "    noised_embedding = noised_embedding.to(m.device)\n",
    "    return noised_embedding\n",
    "\n",
    "def generate_seq(\n",
    "    input_seq, embedding, gen_num=3, fixed_pos=None,\n",
    "    noise_start=0.5, noise_step=0.1, noise_timestep=50, time_limit=500\n",
    "):\n",
    "    total_start = time()\n",
    "    total_step=0\n",
    "    noise_add = 0\n",
    "    res=[]\n",
    "    step=0\n",
    "    while len(res)<gen_num:\n",
    "        # Prepare the encoder outputs with the noised hidden states\n",
    "        noised_embedding = add_noise(\n",
    "            embedding, (noise_start+(noise_add*noise_step)), fixed_pos)\n",
    "        # Prepare the decoder input (usually starts with <pad> token)\n",
    "        decoder_input_ids = tokenizer(\"<pad>\", return_tensors='pt').input_ids\n",
    "        decoder_input_ids = decoder_input_ids.to(device)\n",
    "        # Generate the output sequence using the noised hidden states from the encoder\n",
    "        output_ids = model.generate(\n",
    "            input_ids=decoder_input_ids, \n",
    "            encoder_outputs=(noised_embedding,),\n",
    "            max_length=len(input_seq)+1\n",
    "        )\n",
    "        # Decode the output ids to get the text\n",
    "        output_seq:str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        output_seq = output_seq.replace(\" \", \"\")\n",
    "        # Weird Result check\n",
    "        is_input_weird = has_consecutive_AA(input_seq) or (has_repeated_AA(input_seq))\n",
    "        is_weird = has_consecutive_AA(output_seq) or (has_repeated_AA(output_seq))\n",
    "        # if input is weird, the resuld could be weird\n",
    "        is_weird = is_weird and (not is_input_weird) and (len(output_seq)!=0)\n",
    "        is_fixed = True\n",
    "        if(fixed_pos):\n",
    "            for p in fixed_pos:\n",
    "                is_fixed = is_fixed and (input_seq[p]==output_seq[p])\n",
    "        # Duplication & Existing Check\n",
    "        if (output_seq != input_seq) and (output_seq not in res) and (not is_weird):\n",
    "            if(is_fixed):\n",
    "                print(\"Noised and Reconstructed Output:\", output_seq)\n",
    "                res.append(output_seq)\n",
    "                step=0\n",
    "            else:\n",
    "                print(\"Found Non-Fixed Sequence:\", output_seq)\n",
    "        if(step>noise_timestep):\n",
    "            step=0\n",
    "            noise_add+=1\n",
    "            print(f\"Increasing Noise to: {noise_start+(noise_add*noise_step)}\")\n",
    "        if(total_step>time_limit):\n",
    "            print(\"Reach time limit\")\n",
    "            break\n",
    "        step+=1\n",
    "        total_step+=1\n",
    "        del decoder_input_ids, noised_embedding\n",
    "    print(\"Original Input:\", input_seq)\n",
    "    print(\"Time cost:\", time()-total_start)\n",
    "    print(\"Total step:\", total_step)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noised and Reconstructed Output: SQCSAYFHCMLSVQC\n",
      "Increasing Noise to: 0.6\n",
      "Increasing Noise to: 0.7\n",
      "Noised and Reconstructed Output: CYCSAYFHCMLSVQC\n",
      "Noised and Reconstructed Output: PQCSAYFHCMLSVQC\n",
      "Noised and Reconstructed Output: CYCNAYFHCMLSVQC\n",
      "Noised and Reconstructed Output: SQSSAYFHCMLSVQC\n",
      "Noised and Reconstructed Output: SQCSASLQCSLSVQC\n",
      "Noised and Reconstructed Output: CYCFAYFHCMLSVQC\n",
      "Increasing Noise to: 0.8\n",
      "Noised and Reconstructed Output: SQCSAYFHCMLSAQC\n",
      "Noised and Reconstructed Output: CQCFAYFHCMLSVQC\n",
      "Noised and Reconstructed Output: CYCNTYFHCMLCVQC\n",
      "Original Input: CQCSAYFHCMLSVQC\n",
      "Time cost: 77.73610472679138\n",
      "Total step: 264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SQCSAYFHCMLSVQC',\n",
       " 'CYCSAYFHCMLSVQC',\n",
       " 'PQCSAYFHCMLSVQC',\n",
       " 'CYCNAYFHCMLSVQC',\n",
       " 'SQSSAYFHCMLSVQC',\n",
       " 'SQCSASLQCSLSVQC',\n",
       " 'CYCFAYFHCMLSVQC',\n",
       " 'SQCSAYFHCMLSAQC',\n",
       " 'CQCFAYFHCMLSVQC',\n",
       " 'CYCNTYFHCMLCVQC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"CQCSAYFHCMLSVQC\"\n",
    "embedding = get_embedding(seq)\n",
    "generate_seq(seq, embedding, gen_num=GEN_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
